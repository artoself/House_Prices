{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_description.txt\n",
      "sample_submission.csv\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import some necessary librairies\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"input\"]).decode(\"utf8\")) #check the files available in the directory\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, Ridge, RidgeCV, LassoCV, LassoLarsCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r df_train\n",
    "#%store -r df_test\n",
    "#%store -r y_train\n",
    "#%store -r test_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('input/train.csv')\n",
    "df_test = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (1460, 81)\n",
      "Test set size: (1459, 80)\n",
      "START data processing 2019-08-02 14:09:56.992586\n",
      "(2917, 79)\n",
      "(2917, 86)\n",
      "(2917, 333)\n",
      "X (1458, 333) y (1458,) X_sub (1459, 333)\n",
      "X (1453, 331) y (1453,) X_sub (1459, 331)\n",
      "START ML 2019-08-02 14:09:59.048005\n",
      "TEST score on CV\n",
      "Kernel Ridge score: 0.1024 (0.0143)\n",
      " 2019-08-02 14:10:41.074810\n",
      "Lasso score: 0.1031 (0.0147)\n",
      " 2019-08-02 14:11:13.043255\n",
      "ElasticNet score: 0.1031 (0.0149)\n",
      " 2019-08-02 14:13:28.368453\n",
      "SVR score: 0.1023 (0.0133)\n",
      " 2019-08-02 14:13:43.003430\n",
      "Lightgbm score: 0.1066 (0.0152)\n",
      " 2019-08-02 14:14:02.058686\n",
      "GradientBoosting score: 0.1072 (0.0138)\n",
      " 2019-08-02 14:15:57.403369\n",
      "[14:15:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:16:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:16:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:16:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:17:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:17:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:17:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:17:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:18:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:18:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Xgboost score: 0.1064 (0.0165)\n",
      " 2019-08-02 14:18:55.528026\n",
      "START Fit\n",
      "2019-08-02 14:18:55.528619 StackingCVRegressor\n",
      "[14:21:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:21:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:21:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:22:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:22:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:22:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:23:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "2019-08-02 14:24:17.993290 elasticnet\n",
      "2019-08-02 14:24:33.243191 lasso\n",
      "2019-08-02 14:24:36.603990 ridge\n",
      "2019-08-02 14:24:40.311459 svr\n",
      "2019-08-02 14:24:42.133280 GradientBoosting\n",
      "2019-08-02 14:24:56.145279 xgboost\n",
      "[14:24:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "2019-08-02 14:25:18.220878 lightgbm\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set size:\", df_train.shape)\n",
    "print(\"Test set size:\", df_test.shape)\n",
    "print('START data processing', datetime.now(), )\n",
    "\n",
    "train_ID = df_train['Id']\n",
    "test_ID = df_test['Id']\n",
    "# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "df_train.drop(['Id'], axis=1, inplace=True)\n",
    "df_test.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "# Deleting outliers\n",
    "df_train = df_train[df_train.GrLivArea < 4500]\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "df_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n",
    "y = df_train.SalePrice.reset_index(drop=True)\n",
    "train_features = df_train.drop(['SalePrice'], axis=1)\n",
    "test_features = df_test\n",
    "\n",
    "features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
    "print(features.shape)\n",
    "# Some of the non-numeric predictors are stored as numbers; we convert them into strings \n",
    "features['MSSubClass'] = features['MSSubClass'].apply(str)\n",
    "features['YrSold'] = features['YrSold'].astype(str)\n",
    "features['MoSold'] = features['MoSold'].astype(str)\n",
    "\n",
    "features['Functional'] = features['Functional'].fillna('Typ')\n",
    "features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n",
    "features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n",
    "features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n",
    "features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n",
    "features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n",
    "\n",
    "features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n",
    "\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    features[col] = features[col].fillna(0)\n",
    "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "    features[col] = features[col].fillna('None')\n",
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    features[col] = features[col].fillna('None')\n",
    "\n",
    "features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "objects = []\n",
    "for i in features.columns:\n",
    "    if features[i].dtype == object:\n",
    "        objects.append(i)\n",
    "\n",
    "features.update(features[objects].fillna('None'))\n",
    "\n",
    "features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Filling in the rest of the NA's\n",
    "\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerics = []\n",
    "for i in features.columns:\n",
    "    if features[i].dtype in numeric_dtypes:\n",
    "        numerics.append(i)\n",
    "features.update(features[numerics].fillna(0))\n",
    "\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerics2 = []\n",
    "for i in features.columns:\n",
    "    if features[i].dtype in numeric_dtypes:\n",
    "        numerics2.append(i)\n",
    "\n",
    "skew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "\n",
    "high_skew = skew_features[skew_features > 0.5]\n",
    "skew_index = high_skew.index\n",
    "\n",
    "for i in skew_index:\n",
    "    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n",
    "\n",
    "features = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n",
    "\n",
    "features['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\n",
    "features['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n",
    "\n",
    "features['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n",
    "                                 features['1stFlrSF'] + features['2ndFlrSF'])\n",
    "\n",
    "features['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n",
    "                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n",
    "\n",
    "features['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n",
    "                              features['EnclosedPorch'] + features['ScreenPorch'] +\n",
    "                              features['WoodDeckSF'])\n",
    "\n",
    "# simplified features\n",
    "features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "print(features.shape)\n",
    "final_features = pd.get_dummies(features).reset_index(drop=True)\n",
    "print(final_features.shape)\n",
    "\n",
    "X = final_features.iloc[:len(y), :]\n",
    "X_sub = final_features.iloc[len(X):, :]\n",
    "\n",
    "print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n",
    "\n",
    "outliers = [30, 88, 462, 631, 1322]\n",
    "X = X.drop(X.index[outliers])\n",
    "y = y.drop(y.index[outliers])\n",
    "\n",
    "overfit = []\n",
    "for i in X.columns:\n",
    "    counts = X[i].value_counts()\n",
    "    zeros = counts.iloc[0]\n",
    "    if zeros / len(X) * 100 > 99.94:\n",
    "        overfit.append(i)\n",
    "\n",
    "overfit = list(overfit)\n",
    "overfit.append('MSZoning_C (all)')\n",
    "\n",
    "X = X.drop(overfit, axis=1).copy()\n",
    "X_sub = X_sub.drop(overfit, axis=1).copy()\n",
    "\n",
    "print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n",
    "\n",
    "# ################## ML ########################################\n",
    "print('START ML', datetime.now(), )\n",
    "\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# rmsle\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "# build our model scoring function\n",
    "def cv_rmse(model, X=X):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y,\n",
    "                                    scoring=\"neg_mean_squared_error\",\n",
    "                                    cv=kfolds))\n",
    "    return (rmse)\n",
    "\n",
    "\n",
    "# setup models    \n",
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n",
    "\n",
    "ridge = make_pipeline(RobustScaler(),\n",
    "                      RidgeCV(alphas=alphas_alt, cv=kfolds))\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(),\n",
    "                      LassoCV(max_iter=1e7, alphas=alphas2,\n",
    "                              random_state=42, cv=kfolds))\n",
    "\n",
    "elasticnet = make_pipeline(RobustScaler(),\n",
    "                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n",
    "                                        cv=kfolds, l1_ratio=e_l1ratio))\n",
    "                                        \n",
    "svr = make_pipeline(RobustScaler(),\n",
    "                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =42)\n",
    "                                   \n",
    "\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                                       num_leaves=4,\n",
    "                                       learning_rate=0.01, \n",
    "                                       n_estimators=5000,\n",
    "                                       max_bin=200, \n",
    "                                       bagging_fraction=0.75,\n",
    "                                       bagging_freq=5, \n",
    "                                       bagging_seed=7,\n",
    "                                       feature_fraction=0.2,\n",
    "                                       feature_fraction_seed=7,\n",
    "                                       verbose=-1,\n",
    "                                       #min_data_in_leaf=2,\n",
    "                                       #min_sum_hessian_in_leaf=11\n",
    "                                       )\n",
    "                                       \n",
    "\n",
    "xgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n",
    "                                     max_depth=3, min_child_weight=0,\n",
    "                                     gamma=0, subsample=0.7,\n",
    "                                     colsample_bytree=0.7,\n",
    "                                     objective='reg:linear', nthread=-1,\n",
    "                                     scale_pos_weight=1, seed=27,\n",
    "                                     reg_alpha=0.00006)\n",
    "\n",
    "# stack\n",
    "stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n",
    "                                            gbr, xgboost, lightgbm),\n",
    "                                meta_regressor=xgboost,\n",
    "                                use_features_in_secondary=True)\n",
    "                                \n",
    "\n",
    "print('TEST score on CV')\n",
    "\n",
    "score = cv_rmse(ridge)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(lasso)\n",
    "print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(elasticnet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(svr)\n",
    "print(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(lightgbm)\n",
    "print(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(gbr)\n",
    "print(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(xgboost)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "\n",
    "print('START Fit')\n",
    "print(datetime.now(), 'StackingCVRegressor')\n",
    "stack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n",
    "print(datetime.now(), 'elasticnet')\n",
    "elastic_model_full_data = elasticnet.fit(X, y)\n",
    "print(datetime.now(), 'lasso')\n",
    "lasso_model_full_data = lasso.fit(X, y)\n",
    "print(datetime.now(), 'ridge')\n",
    "ridge_model_full_data = ridge.fit(X, y)\n",
    "print(datetime.now(), 'svr')\n",
    "svr_model_full_data = svr.fit(X, y)\n",
    "print(datetime.now(), 'GradientBoosting')\n",
    "gbr_model_full_data = gbr.fit(X, y)\n",
    "print(datetime.now(), 'xgboost')\n",
    "xgb_model_full_data = xgboost.fit(X, y)\n",
    "print(datetime.now(), 'lightgbm')\n",
    "lgb_model_full_data = lightgbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a cross validation strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 12\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, df_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LASSO Regression** :\n",
    "\n",
    "This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Elastic Net Regression :**\n",
    "\n",
    "again made robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Kernel Ridge Regression :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient Boosting Regression** :\n",
    "\n",
    "With huber loss that makes it robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **XGBoost :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LightGBM :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base models scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these base models perform on the data by evaluating the cross-validation rmsle error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score: 0.1106 (0.0146)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet score: 0.1104 (0.0147)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge score: 0.1121 (0.0142)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting score: 0.1141 (0.0144)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost score: 0.1134 (0.0147)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM score: 0.1129 (0.0140)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplest Stacking approach : Averaging base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Averaged base models class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Averaged base models score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Averaged base models score: 0.1069 (0.0145)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n",
    "\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow ! It seems even the simplest stacking approach really improve the score . This encourages us to go further and explore a less simple stacking approch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Less simple Stacking : Adding a Meta-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking averaged Models Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking Averaged models Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the two approaches comparable (by using the same number of models) , we just average Enet KRR and Gboost, then we add lasso as meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Averaged models score: 0.1063 (0.0147)\n"
     ]
    }
   ],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
    "                                                 meta_model = lasso)\n",
    "\n",
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling StackedRegressor, XGBoost and LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add XGBoost and LightGBM to the StackedRegressor defined previously.\n",
    "\n",
    "\n",
    "\n",
    "We first define a rmsle evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Training and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StackedRegressor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08000699307806071\n"
     ]
    }
   ],
   "source": [
    "stacked_averaged_models.fit(df_train.values, y_train)\n",
    "stacked_train_pred = stacked_averaged_models.predict(df_train.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(df_test.values))\n",
    "print(rmsle(y_train, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08016272793094772\n"
     ]
    }
   ],
   "source": [
    "model_xgb.fit(df_train, y_train)\n",
    "xgb_train_pred = model_xgb.predict(df_train)\n",
    "xgb_pred = np.expm1(model_xgb.predict(df_test))\n",
    "print(rmsle(y_train, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGBM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07439208865855762\n"
     ]
    }
   ],
   "source": [
    "model_lgb.fit(df_train, y_train)\n",
    "lgb_train_pred = model_lgb.predict(df_train)\n",
    "lgb_pred = np.expm1(model_lgb.predict(df_test.values))\n",
    "print(rmsle(y_train, lgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE score on train data:\n",
      "0.07733735049395318\n"
     ]
    }
   ],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(y_train,stacked_train_pred*0.70 +\n",
    "               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, df_train, y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'rmse')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n",
    "            for alpha in alphas]\n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "cv_ridge.plot(title = \"Validation - Just Do It\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the U-ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible (alpha small) the model begins to overfit. A value of alpha = 10 is about right based on the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11614232852470793"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ridge.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11458209008399503"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_cv(model_lasso).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(model_lasso.coef_, index = df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso picked 81 variables and eliminated the other 138 variables\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = pd.concat([coef.sort_values().head(10),\n",
    "                     coef.sort_values().tail(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(df_train, label = y_train)\n",
    "dtest = xgb.DMatrix(df_test)\n",
    "\n",
    "params = {\"max_depth\":2, \"eta\":0.1}\n",
    "model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:31:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=1, missing=None, n_estimators=360,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\n",
    "model_xgb.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds = np.expm1(model_xgb.predict(df_test))\n",
    "lasso_preds = np.expm1(model_lasso.predict(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = 0.7*lasso_preds + 0.3*xgb_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\"id\":test_ID, \"SalePrice\":preds})\n",
    "solution.to_csv(\"ridge_sol.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = ensemble\n",
    "sub.to_csv('submission_02.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = lgb_pred\n",
    "sub.to_csv('lgb_sol.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = xgb_pred\n",
    "sub.to_csv('xgb_sol.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
